@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{song2021maximum,
  title={Maximum likelihood training of score-based diffusion models},
  author={Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1415--1428},
  year={2021}
}

@article{alonso2024diffusion,
  title={Diffusion for World Modeling: Visual Details Matter in Atari},
  author={Alonso, Eloi and Jelley, Adam and Micheli, Vincent and Kanervisto, Anssi and Storkey, Amos and Pearce, Tim and Fleuret, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2405.12399},
  year={2024}
}

@article{papoudakis2020comparative,
  title={Comparative evaluation of cooperative multi-agent deep reinforcement learning algorithms},
  author={Papoudakis, Georgios and Christianos, Filippos and Sch{\"a}fer, Lukas and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2006.07869},
  year={2020}
}

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International conference on machine learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{hughes2018inequity,
  title={Inequity aversion improves cooperation in intertemporal social dilemmas},
  author={Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due{\~n}ez-Guzman, Edgar and Garc{\'\i}a Casta{\~n}eda, Antonio and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{schafer2022learning,
  title={Learning task embeddings for teamwork adaptation in multi-agent reinforcement learning},
  author={Sch{\"a}fer, Lukas and Christianos, Filippos and Storkey, Amos and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2207.02249},
  year={2022}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{vahdat2021score,
  title={Score-based generative modeling in latent space},
  author={Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11287--11302},
  year={2021}
}

@article{salimans2022progressive,
  title={Progressive distillation for fast sampling of diffusion models},
  author={Salimans, Tim and Ho, Jonathan},
  journal={arXiv preprint arXiv:2202.00512},
  year={2022}
}

@article{huang2022riemannian,
  title={Riemannian diffusion models},
  author={Huang, Chin-Wei and Aghajohari, Milad and Bose, Joey and Panangaden, Prakash and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2750--2761},
  year={2022}
}

@article{chung2022improving,
  title={Improving diffusion models for inverse problems using manifold constraints},
  author={Chung, Hyungjin and Sim, Byeongsu and Ryu, Dohoon and Ye, Jong Chul},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25683--25696},
  year={2022}
}

@article{de2022riemannian,
  title={Riemannian score-based generative modelling},
  author={De Bortoli, Valentin and Mathieu, Emile and Hutchinson, Michael and Thornton, James and Teh, Yee Whye and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2406--2422},
  year={2022}
}

@misc{thomasTensorFieldNetworks2018,
  title = {Tensor Field Networks: {{Rotation-}} and Translation-Equivariant Neural Networks for {{3D}} Point Clouds},
  shorttitle = {Tensor Field Networks},
  author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  year = {2018},
  month = may,
  number = {arXiv:1802.08219},
  eprint = {1802.08219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.08219},
  urldate = {2025-07-04},
  abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{weiler3DSteerableCNNs2018,
  title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
  shorttitle = {{{3D Steerable CNNs}}},
  author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  year = {2018},
  month = oct,
  number = {arXiv:1807.02547},
  eprint = {1807.02547},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.02547},
  urldate = {2025-07-04},
  abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R{\textasciicircum}3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kondorClebschGordanNetsFully2018,
  title = {Clebsch-{{Gordan Nets}}: A {{Fully Fourier Space Spherical Convolutional Neural Network}}},
  shorttitle = {Clebsch-{{Gordan Nets}}},
  author = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  year = {2018},
  month = nov,
  number = {arXiv:1806.09231},
  eprint = {1806.09231},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.09231},
  urldate = {2025-07-04},
  abstract = {Recent work by Cohen {\textbackslash}emph\{et al.\} has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{hyvarinenEstimationNonNormalizedStatistical2005,
  title = {Estimation of {{Non-Normalized Statistical Models}} by {{Score Matching}}},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {24},
  pages = {695--709},
  issn = {1533-7928},
  urldate = {2025-07-04},
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.}
}

@article{vincentConnectionScoreMatching2011,
  title = {A {{Connection Between Score Matching}} and {{Denoising Autoencoders}}},
  author = {Vincent, Pascal},
  year = {2011},
  month = jul,
  journal = {Neural Computation},
  volume = {23},
  number = {7},
  pages = {1661--1674},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00142},
  urldate = {2025-07-04},
  abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.}
}

@article{andersonReversetimeDiffusionEquation1982,
  title = {Reverse-Time Diffusion Equation Models},
  author = {Anderson, Brian D. O.},
  year = {1982},
  month = may,
  journal = {Stochastic Processes and their Applications},
  volume = {12},
  number = {3},
  pages = {313--326},
  issn = {0304-4149},
  doi = {10.1016/0304-4149(82)90051-5},
  urldate = {2025-07-04},
  abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.}
}

@book{leimkuhlerMolecularDynamicsDeterministic2015,
  title = {Molecular {{Dynamics}}: {{With Deterministic}} and {{Stochastic Numerical Methods}}},
  shorttitle = {Molecular {{Dynamics}}},
  author = {Leimkuhler, Ben and Matthews, Charles},
  year = {2015},
  series = {Interdisciplinary {{Applied Mathematics}}},
  volume = {39},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-16375-8},
  urldate = {2025-07-06},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-319-16374-1 978-3-319-16375-8},
  langid = {english},
  keywords = {65C2065P1065Z0582C3182C0582B80,biomolecular simulation,computational physics,materials modelling,molecular dynamics,theoretical chemistry}
}
