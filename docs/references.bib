%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Diffusion Models: DDPM, SBD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DDPM
@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-08-22},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.}
}

% Cosine Schedule
@misc{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  year = {2021},
  month = feb,
  number = {arXiv:2102.09672},
  eprint = {2102.09672},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09672},
  urldate = {2025-07-13},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

% Reverse SDE
@article{andersonReversetimeDiffusionEquation1982,
  title = {Reverse-Time Diffusion Equation Models},
  author = {Anderson, Brian D. O.},
  year = {1982},
  month = may,
  journal = {Stochastic Processes and their Applications},
  volume = {12},
  number = {3},
  pages = {313--326},
  issn = {0304-4149},
  doi = {10.1016/0304-4149(82)90051-5},
  urldate = {2025-07-04},
  abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.}
}

% Score Matching
@article{hyvarinenEstimationNonNormalizedStatistical2005,
  title = {Estimation of {{Non-Normalized Statistical Models}} by {{Score Matching}}},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {24},
  pages = {695--709},
  issn = {1533-7928},
  urldate = {2025-07-04},
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.}
}

% Denoising Score Matching
@article{vincentConnectionScoreMatching2011,
  title = {A {{Connection Between Score Matching}} and {{Denoising Autoencoders}}},
  author = {Vincent, Pascal},
  year = {2011},
  month = jul,
  journal = {Neural Computation},
  volume = {23},
  number = {7},
  pages = {1661--1674},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00142},
  urldate = {2025-07-04},
  abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.}
}

% Random Fourier Features
@inproceedings{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  year = {2020},
  volume = {33},
  pages = {7537--7547},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-23},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.}
}

% SBD
@inproceedings{song2021ScoreBasedGenerativeModeling,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = jan,
  urldate = {2025-08-23},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of \$1024{\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
  langid = {english}
}

@inproceedings{songMaximumLikelihoodTraining2021,
  title = {Maximum {{Likelihood Training}} of {{Score-Based Diffusion Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  year = {2021},
  volume = {34},
  pages = {1415--1428},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-08-22}
}

@article{saharia2022PhotorealisticTexttoImageDiffusion,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L. and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {36479--36494},
  urldate = {2025-08-25},
  langid = {english}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% VAE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{kingma2022AutoEncodingVariationalBayes,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6114},
  urldate = {2025-08-22},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GAN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{goodfellow2020GenerativeAdversarialNetworks,
  title = {Generative Adversarial Networks},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2020},
  month = oct,
  journal = {Commun. ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  issn = {0001-0782},
  doi = {10.1145/3422622},
  urldate = {2025-08-22},
  abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.}
}

@inproceedings{dhariwal2021DiffusionModelsBeat,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-08-22}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction to Protein Structures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@book{creighton1993proteins,
  title={Proteins: structures and molecular properties},
  author={Creighton, Thomas E},
  year={1993},
  publisher={Macmillan}
}

@book{branden2012IntroductionProteinStructure,
  title = {Introduction to {{Protein Structure}}},
  author = {Branden, Carl Ivar and Tooze, John},
  year = {2012},
  month = mar,
  edition = {2},
  publisher = {Garland Science},
  address = {New York},
  doi = {10.1201/9781136969898},
  abstract = {The VitalBook e-book of Introduction to Protein Structure, Second Edition is inly available in the US and Canada at the present time. To purchase or rent please visit https://store.vitalsource.com/show/9780815323051Introduction to Protein Structure provides an account of the principles of protein structure, with examples of key proteins in their bio},
  isbn = {978-0-429-06209-4}
}

@book{alberts2015essential,
  title={Essential cell biology},
  author={Alberts, Bruce and Bray, Dennis and Hopkin, Karen and Johnson, Alexander D and Lewis, Julian and Raff, Martin and Roberts, Keith and Walter, Peter},
  year={2015},
  publisher={Garland Science}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Protein Generation with Diffusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{jumper2021HighlyAccurateProtein,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2025-08-22},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology}
}

@article{watsonNovoDesignProtein2023,
  title = {De Novo Design of Protein Structure and Function with {{RFdiffusion}}},
  author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year = {2023},
  month = aug,
  journal = {Nature},
  volume = {620},
  number = {7976},
  pages = {1089--1100},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06415-8},
  urldate = {2025-06-29},
  abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1--9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence--structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein design,Proteins}
}

@article{wuProteinStructureGeneration2024,
  title = {Protein Structure Generation via Folding Diffusion},
  author = {Wu, Kevin E. and Yang, Kevin K. and {van den Berg}, Rianne and Alamdari, Sarah and Zou, James Y. and Lu, Alex X. and Amini, Ava P.},
  year = {2024},
  month = feb,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {1059},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45051-2},
  urldate = {2025-06-29},
  abstract = {The ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases. Despite recent advances in protein structure prediction, directly generating diverse, novel protein structures from neural networks remains difficult. In this work, we present a diffusion-based generative model that generates protein backbone structures via a procedure inspired by the natural folding process. We describe a protein backbone structure as a sequence of angles capturing the relative orientation of the constituent backbone atoms, and generate structures by denoising from a random, unfolded state towards a stable folded structure. Not only does this mirror how proteins natively twist into energetically favorable conformations, the inherent shift and rotational invariance of this representation crucially alleviates the need for more complex equivariant networks. We train a denoising diffusion probabilistic model with a simple transformer backbone and demonstrate that our resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins. As a useful resource, we release an open-source codebase and trained models for protein structure diffusion.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Biochemistry,Biophysics,Computational models,Protein design}
}

@article{yimDiffusionModelsProtein2024,
  title = {Diffusion Models in Protein Structure and Docking},
  author = {Yim, Jason and St{\"a}rk, Hannes and Corso, Gabriele and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi S.},
  year = {2024},
  journal = {WIREs Computational Molecular Science},
  volume = {14},
  number = {2},
  pages = {e1711},
  issn = {1759-0884},
  doi = {10.1002/wcms.1711},
  urldate = {2025-06-29},
  abstract = {Generative AI is rapidly transforming the frontier of research in computational structural biology. Indeed, recent successes have substantially advanced protein design and drug discovery. One of the key methodologies underlying these advances is diffusion models (DM). Diffusion models originated in computer vision, rapidly taking over image generation and offering superior quality and performance. These models were subsequently extended and modified for uses in other areas including computational structural biology. DMs are well equipped to model high dimensional, geometric data while exploiting key strengths of deep learning. In structural biology, for example, they have achieved state-of-the-art results on protein 3D structure generation and small molecule docking. This review covers the basics of diffusion models, associated modeling choices regarding molecular representations, generation capabilities, prevailing heuristics, as well as key limitations and forthcoming refinements. We also provide best practices around evaluation procedures to help establish rigorous benchmarking and evaluation. The review is intended to provide a fresh view into the state-of-the-art as well as highlight its potentials and current challenges of recent generative techniques in computational structural biology. This article is categorized under: Data Science {$>$} Artificial Intelligence/Machine Learning Structure and Mechanism {$>$} Molecular Structures Software {$>$} Molecular Modeling},
  copyright = {{\copyright} 2024 The Authors. WIREs Computational Molecular Science published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {diffusion models,docking,generative models,proteins}
}

@article{senior2020ImprovedProteinStructure,
  title = {Improved Protein Structure Prediction Using Potentials from Deep Learning},
  author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v Z}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7792},
  pages = {706--710},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1923-7},
  urldate = {2025-08-25},
  abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)---a blind assessment of the state of the field---AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Machine learning,Protein structure predictions}
}

% TMalign
@article{zhang2005TMalignProteinStructure,
  title = {{{TM-align}}: A Protein Structure Alignment Algorithm Based on the {{TM-score}}},
  shorttitle = {{{TM-align}}},
  author = {Zhang, Yang and Skolnick, Jeffrey},
  year = {2005},
  month = apr,
  journal = {Nucleic Acids Research},
  volume = {33},
  number = {7},
  pages = {2302--2309},
  issn = {0305-1048},
  doi = {10.1093/nar/gki524},
  urldate = {2025-08-25},
  abstract = {We have developed TM-align, a new algorithm to identify the best structural alignment between protein pairs that combines the TM-score rotation matrix and Dynamic Programming (DP). The algorithm is {$\sim$}4 times faster than CE and 20 times faster than DALI and SAL. On average, the resulting structure alignments have higher accuracy and coverage than those provided by these most often-used methods. TM-align is applied to an all-against-all structure comparison of 10\,515 representative protein chains from the Protein Data Bank (PDB) with a sequence identity cutoff \&lt;95\%: 1996 distinct folds are found when a TM-score threshold of 0.5 is used. We also use TM-align to match the models predicted by TASSER for solved non-homologous proteins in PDB. For both folded and misfolded models, TM-align can almost always find close structural analogs, with an average root mean square deviation, RMSD, of 3 {\AA} and 87\% alignment coverage. Nevertheless, there exists a significant correlation between the correctness of the predicted structure and the structural similarity of the model to the other proteins in the PDB. This correlation could be used to assist in model selection in blind protein structure predictions. The TM-align program is freely downloadable at http://bioinformatics.buffalo.edu/TM-align .}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GNN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  urldate = {2025-07-23},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2025-07-23},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning}
}

@inproceedings{cai2021GraphNormPrincipledApproach,
  title = {{{GraphNorm}}: {{A Principled Approach}} to {{Accelerating Graph Neural Network Training}}},
  shorttitle = {{{GraphNorm}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Cai, Tianle and Luo, Shengjie and Xu, Keyulu and He, Di and Liu, Tie-Yan and Wang, Liwei},
  year = {2021},
  month = jul,
  pages = {1204--1215},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-08-23},
  abstract = {Normalization is known to help the optimization of deep neural networks. Curiously, different architectures require specialized normalization methods. In this paper, we study what normalization is effective for Graph Neural Networks (GNNs). First, we adapt and evaluate the existing methods from other domains to GNNs. Faster convergence is achieved with InstanceNorm compared to BatchNorm and LayerNorm. We provide an explanation by showing that InstanceNorm serves as a preconditioner for GNNs, but such preconditioning effect is weaker with BatchNorm due to the heavy batch noise in graph datasets. Second, we show that the shift operation in InstanceNorm results in an expressiveness degradation of GNNs for highly regular graphs. We address this issue by proposing GraphNorm with a learnable shift. Empirically, GNNs with GraphNorm converge faster compared to GNNs using other normalization. GraphNorm also improves the generalization of GNNs, achieving better performance on graph classification benchmarks.},
  langid = {english}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PyG
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{PyG1.0,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@inproceedings{PyG2.0,
  title={{PyG} 2.0: Scalable Learning on Real World Graphs},
  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan and Stojanovi{\v{c}}, Bla{\v{z}} and Bendias, Ramona and Alexandria, Barghi and Kocijan, Vid and Zhang, Zecheng and He, Xinwei and Lenssen, Jan E. and Leskovec, Jure},
  booktitle={Temporal Graph Learning Workshop @ KDD},
  year={2025},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PyMOL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@unpublished{AxPyMOL,
	Author = {{Schr\"odinger, LLC}},
	Date-Added = {2010-08-19 17:58:20 -0400},
	Date-Modified = {2015-12-22 18:04:08 -0400},
	Month = {November},
	Title = {The {AxPyMOL} Molecular Graphics Plugin for {Microsoft PowerPoint}, Version~1.8},
	Year = {2015}}

@unpublished{JyMOL,
	Author = {{Schr\"odinger, LLC}},
	Date-Added = {2010-08-19 17:55:56 -0400},
	Date-Modified = {2015-12-22 18:04:08 -0400},
	Month = {November},
	Title = {The {JyMOL} Molecular Graphics Development Component, Version~1.8},
	Year = {2015}}

@unpublished{PyMOL,
	Annote = {PyMOL

The PyMOL Molecular Graphics System, Version 1.8, Schr{\"o}dinger, LLC.},
	Author = {{Schr\"odinger, LLC}},
	Date-Added = {2010-08-19 17:29:55 -0400},
	Date-Modified = {2015-12-22 18:04:08 -0400},
	Month = {November},
	Title = {The {PyMOL} Molecular Graphics System, Version~1.8},
	Year = {2015}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% E3NN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{thomasTensorFieldNetworks2018,
  title = {Tensor Field Networks: {{Rotation-}} and Translation-Equivariant Neural Networks for {{3D}} Point Clouds},
  shorttitle = {Tensor Field Networks},
  author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  year = {2018},
  month = may,
  number = {arXiv:1802.08219},
  eprint = {1802.08219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.08219},
  urldate = {2025-07-04},
  abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{weiler3DSteerableCNNs2018,
  title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
  shorttitle = {{{3D Steerable CNNs}}},
  author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  year = {2018},
  month = oct,
  number = {arXiv:1807.02547},
  eprint = {1807.02547},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.02547},
  urldate = {2025-07-04},
  abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R{\textasciicircum}3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kondorClebschGordanNetsFully2018,
  title = {Clebsch-{{Gordan Nets}}: A {{Fully Fourier Space Spherical Convolutional Neural Network}}},
  shorttitle = {Clebsch-{{Gordan Nets}}},
  author = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  year = {2018},
  month = nov,
  number = {arXiv:1806.09231},
  eprint = {1806.09231},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.09231},
  urldate = {2025-07-04},
  abstract = {Recent work by Cohen {\textbackslash}emph\{et al.\} has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{geiger2022E3nnEuclideanNeural,
  title = {E3nn: {{Euclidean Neural Networks}}},
  shorttitle = {E3nn},
  author = {Geiger, Mario and Smidt, Tess},
  year = {2022},
  month = jul,
  number = {arXiv:2207.09453},
  eprint = {2207.09453},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.09453},
  urldate = {2025-08-23},
  abstract = {We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{geiger2025EuclideanNeuralNetworks,
  title = {Euclidean Neural Networks: E3nn},
  shorttitle = {E3nn/E3nn},
  author = {Geiger, Mario and Smidt, Tess and M, Alby and Miller, Benjamin Kurt and Boomsma, Wouter and Dice, Bradley and Lapchevskyi, Kostiantyn and Kotak, Mit and Tyszkiewicz, Micha{\l} and Uhrin, Martin and Batzner, Simon and Madisetti, Dylan and Frellsen, Jes and Maheshkar, Saurav and Jung, Nuri and Fomitchev, Boris and Wen, Mingjian and {jkh} and Sanborn, Sophia and Barnes, Richard and Kohler, Colin and Morehead, Alex and Tan, Chuin Wei and Rackers, Josh and LFu and R{\o}d, Marcel and Bailey, Michael and Brocidiacono, Michael and {eszter137} and McConkey, Ryley},
  year = {2025},
  month = mar,
  doi = {10.5281/zenodo.15069471},
  urldate = {2025-08-23},
  abstract = {What's Changed Refactor by @cw-tan in https://github.com/e3nn/e3nn/pull/502 0.5.6 version bump by @mitkotak in https://github.com/e3nn/e3nn/pull/504 New Contributors @cw-tan made their first contribution in https://github.com/e3nn/e3nn/pull/502 Full Changelog: https://github.com/e3nn/e3nn/compare/0.5.5...0.5.6},
  howpublished = {Zenodo}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Transformers, Attention Layers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.}
}
