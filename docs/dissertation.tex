\documentclass[12pt]{report}
\title{Constrained Diffusion Models}
\date{\today}
\author{Yuanhao JIANG}
\flushbottom
\usepackage{
    abraces, algorithm, algpseudocode, amsfonts, amsmath, amssymb, amsthm, array, bm, cite, diagbox, enumitem, graphicx, geometry, mathtools, subcaption, threeparttable
}
\usepackage[raggedright]{titlesec}
\usepackage[breaklinks]{hyperref}
\usepackage{cleveref}
\geometry{margin=1in}
% \graphicspath{folderA}

% allow page breaks inside equations
\allowdisplaybreaks

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]

\DeclareMathOperator{\Arg}{Arg} % Branch argument of complex numbers
\DeclareMathOperator{\Var}{Var} % variance
\DeclareMathOperator{\Cov}{Cov} % covariance
\DeclareMathOperator{\corr}{corr} % correlation
\DeclareMathOperator{\Int}{Int} % interior
\DeclareMathOperator{\Ext}{Ext} % exterior
\DeclareMathOperator{\Res}{Res} % residule
\DeclareMathOperator{\tr}{tr} % trace
\DeclareMathOperator*{\argmax}{arg\,max} % argmax
\DeclareMathOperator*{\argmin}{arg\,min} % argmin
\DeclarePairedDelimiter\abs{\lvert}{\rvert} % absolute value
\DeclarePairedDelimiter\norm{\lVert}{\rVert} % norm

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}


\begin{document}
\pagenumbering{roman}
\maketitle

\begin{abstract}
    Abstract
\end{abstract}

\tableofcontents
\addcontentsline{toc}{chapter}{Contents}
\newpage
\pagenumbering{arabic}

\chapter{paperworks}
\section{Data}
CATH S40 domain structures, it contains non-redundant data with no pair of domains with \(\geq 40\%\) sequence similarity (according to BLAST).

Extract the \(C\alpha\) (alpha carbon) atoms for each protein domain in the dataset, so each data is a 3D point cloud of \(C\alpha\) atoms
\begin{align*}
    x = \left[x_1,x_2,...,x_l\right],\quad x_i\in\mathbb{R}^3\quad\forall i=1,...,l,
\end{align*}
where \(l\) the number of \(C\alpha\) atoms (residues) is varying for different domain.
\section{Model Architecture}
\subsection{Message Passing Layers}
\subsubsection{E3NN}
Euclidean neural networks \cite{thomasTensorFieldNetworks2018,weiler3DSteerableCNNs2018,kondorClebschGordanNetsFully2018} is rotation and translation invariant for 3D point cloud, and all its operations are per-node, or per-edge based, so it is compatible with variable-sized structures.
\subsubsection{Graph Neural Networks}
Graph Neural Networks (GNNs) were first introduced in the seminal work by \cite{scarselliGraphNeuralNetwork2009}, which proposed a recursive framework for learning over graphs. The field progressed significantly with the development of Graph Convolutional Networks (GCNs) by \cite{kipfSemiSupervisedClassificationGraph2017}, which introduced a scalable and differentiable message-passing scheme. Further, Graph Attention Networks \cite{velickovicGraphAttentionNetworks2018}, brought in attention mechanisms to improve expressiveness and performance. To better model the complicated structure, I use the graph transformer network \cite{shiMaskedLabelPrediction2021} which incorporates multi-head self-attention over local neighborhoods.
\subsection{Time Conditioning via Gaussian Fourier Features}
Following \cite{song2020score}, we embed the diffusion time step \(t\in\left(0,1\right)\) using random Fourier features \cite{tancikFourierFeaturesLet2020}
\begin{align*}
    \gamma\left(t\right)=\left[\cos\left(2\pi Wt\right),\sin\left(2\pi Wt\right)\right],\quad W\sim\mathcal{N}\left(0,s^2\right),
\end{align*}
to provide continuous and expressive encoding of time.
\subsection{Positional Encoding for Node Order Awareness}
Graph-based models are inherently invariant to node permutations, which can be a limitation when modeling protein backbones, where the sequential order of residues encodes biologically meaningful directionality along the chain. To inject this ordering information, each node is assigned a scalar index \(p\in\left[0,1\right]\) representing its normalized position in the sequence. This index is first mapped to a high-dimensional representation using a sinusoidal encoding \cite{vaswaniAttentionAllYou2017}
\begin{align*}
    \rho\left(p\right)=\left[\sin\left(\omega_1 p\right),\cos\left(\omega_1 p\right),\sin\left(\omega_2 p\right),\cos\left(\omega_2 p\right),...\right]
\end{align*}
where \(w_k\)'s are fixed frequencies. The resulting positional embedding is then passed through a learnable transformation, such as a multilayer perceptron (MLP), before being incorporated into the model
\subsection{Residual Network Architecture}
Overall residual connections are used at each GNN layer.
\section{Score-Based Diffusion Models}
\subsection{Forward Process}
Use Variance Preserving (VP) SDE \cite{song2020score}
\begin{align}\label{eq:VPSDE}
    dx=-\frac{1}{2}\beta(t)x+\sqrt{\beta(t)}dw
\end{align}
with closed form posterior
\begin{align*}
    p\left(x_t|x_0\right)=\mathcal{N}\left(x_t:\:x_0e^{-\frac{1}{2}\int_0^t\beta(s)ds},\left(1-e^{-\int_0^t\beta(s)ds}\right)I\right)
\end{align*}
for data perturbation, i.e.,
\begin{align}\label{eq:perturbation_ddpm}
    x_t=\sqrt{e^{-\int_0^t\beta(s)ds}}x_0+\sqrt{1-e^{-\int_0^t\beta(s)ds}}z_t,\quad z_t\sim\mathcal{N}\left(0, I\right).
\end{align}
Notice that \cref{eq:perturbation_ddpm} is exactly the discrete perturbation scheme in DDPM \cite{ho2020denoising}. In practice, linear \(\beta(t)\) as used in \cite{song2020score} perturb the backbone too fast, the perturbed samples are nearly random noise after a small time \(t\). To slow down perturbation, I used a cosine noise schedule as proposed in \cite{nicholImprovedDenoisingDiffusion2021}, where
\begin{align*}
    x_t=\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}z_t,\quad\alpha_t=\cos^2\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right),\quad z_t\sim\mathcal{N}\left(0, I\right).
\end{align*}
This is a DDPM perturbation scheme, the corresponding SDE is given by solving
\begin{align*}
    \alpha_t=\cos^2\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)=e^{-\int_0^t\beta(s)ds},
\end{align*}
it follows that
\begin{align*}
    \beta(t)
    &=-\frac{d}{dt}\log\cos^2\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)\\
    &=-\frac{1}{\cos^2\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)}\left[2\cos\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)\right]\left[-\sin\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)\right]\frac{d}{dt}\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right)\\
    &=\frac{\pi}{1+s}\tan\left(\frac{t+s}{1+s}\cdot\frac{\pi}{2}\right).
\end{align*}
\subsection{Denoising Score Matching}
Ideally, for each time \(t\), the score function of \(x_t\) can be trained via
\begin{align*}
    \mathbb{E}_{p\left(x_t\right)}\left[\norm{\nabla_{x_t}\log p\left(x_t\right)-s_{\theta}\left(x_t, t\right)}_2^2\right].
\end{align*}
The unknown term \(\nabla_{x_t}\log p\left(x_t\right)\) (true score) can be eliminated with the score matching objective \cite{hyvarinenEstimationNonNormalizedStatistical2005}
\begin{align*}
    J^{\text{SM}}_t(\theta)=\mathbb{E}_{p\left(x_t\right)}\left[\norm{\tr\left(\nabla_{x_t}s_{\theta}\left(x_t,t\right)\right)-\frac{1}{2}\norm{s_{\theta}(x_t,t)}_2^2}_2^2\right].
\end{align*}
Note that the term \(\tr\left(\nabla_{x_t}s_{\theta}\left(x_t,t\right)\right)\) can be computationally intensive, since we have the analytic form of the posterior, this can be eased by using instead the denoising score matching objective \cite{vincentConnectionScoreMatching2011}
\begin{align*}
    J^{\text{DSM}}_t=\mathbb{E}_{p\left(x_0\right)}\mathbb{E}_{p\left(x_t|x_0\right)}\left[\norm{s_{\theta}\left(x_t,t\right)-\nabla_{x_t}\log p\left(x_t|x_0\right)}_2^2\right].
\end{align*}
The overall objective is given by a weighted sum (or average)
\begin{align*}
    J^{\text{DSM}}=\mathbb{E}_{t\sim\mathcal{U}(0,1)}\left[\lambda\left(t\right)J^{\text{DSM}}_t\right].
\end{align*}
\subsection{Reverse Process}
For an SDE
\begin{align*}
    dx=f(x,t)dt+g(t)dw,
\end{align*}
the reverse time SDE is given by \cite{andersonReversetimeDiffusionEquation1982}
\begin{align*}
    dx=\left[f(x,t)-g^2(t)\nabla_{x}\log p\left(x_t\right)\right]dt+g\left(t\right)d\tilde{w},
\end{align*}
where \(t\) goes from \(1\) to \(0\). So for VPSDE \eqref{eq:VPSDE} its reverse process is
\begin{align*}
    dx=\left[-\frac{1}{2}\beta(t)-\beta^2(t)\nabla_{x}\log p\left(x_t\right)\right]dt+\sqrt{\beta(t)}d\tilde{w},
\end{align*}
and the true score is approximated by the trained neural network \(s_{\theta}\). For discretisation the Predictor-Corrector (PC) sampler \cite{song2020score} is used, i.e., the predictor step steps into next time step in the reverse SDE
\begin{align*}
    x_{k+1}\gets x_k+\left[f(x,t)-g^2(t)s_\theta\left(x_k, t\right)\right]\left(-\delta t\right)+g\left(t\right)\sqrt{\delta t}z_{k+1},
\end{align*}
and the corrector step corrects the sampling using MCMC method in a fixed time \(t\)
\begin{align*}
    x_{k+1}\gets x_{k+1}+\epsilon_{k+1}s_{\theta}\left(x_{k+1},t\right)+\sqrt{2}z_{k+1}',
\end{align*}
for white noises \(z_{k+1}\) and \(z_{k+1}'\).
\section{Reimannian Score-Based Diffusion Models}
The constraints I use are
\begin{enumerate}
    \item fixed bond lengths (\(C\alpha\)-\(C\alpha\) distances),
        \begin{align*}
            \norm{x_{i+1}-x_{i}}=\text{const};
        \end{align*}
    \item fixed bond angles,
        \begin{align*}
            \angle\left(x_{i-1},x_{i},x_{i+1}\right)=\text{const};
        \end{align*}
    \item ....
\end{enumerate}
Suppose, for an \(x\in\mathbb{R}^{l \times 3}\) on the manifold, its constraints are expressed as
\begin{align*}
    h_i(x)=0,\quad i=1,...,m,
\end{align*}
the Jacobian matrix of the constraints is given by
\begin{align*}
    J(x)=\begin{bmatrix}
        \nabla_{x}^T h_1\left(x\right)\\
        \nabla_{x}^T h_2\left(x\right)\\
        \vdots\\
        \nabla_{x}^T h_m\left(x\right)
    \end{bmatrix}\in\mathbb{R}^{m\times3l}.
\end{align*}
Since now we have no closed form solution to the SDE, a discretisation rule is need to perturb the data.
\subsection{Geodesic Random Walk}
The projector of the tangent space of \(x\) is given by
\begin{align*}
    \Pi(x)=I-J^T(x)(J(x)J^T(x))^{-1}J(x),
\end{align*}
so the SDE on manifold is given by
\begin{align}\label{eq:constrained-sde-tangent-space}
    dx = \Pi(x)f(x,t)dt+\Pi(x)g(t)dw.
\end{align}
To sample from it, we follows the geodesic random walk in \cite{de2022riemannian}. At time \(t\), first sample on tangent space of \(x_t\)
\begin{align*}
    v_{k+1}=\Pi\left(x_t\right)\left(f(x_t,t)\delta t+g(t)\sqrt{\delta t}z_{k+1}\right),
\end{align*}
then move along the geodesic on manifold \(\mathcal{M}\) defined by the constraints, i.e.,
\begin{align*}
    x_{k+1}\gets\exp_{x_k}\left(v_{k+1}\right).
\end{align*}
The exponential map \(\exp_{x}\left(v\right)\) is known for sphere and torus in \cite{de2022riemannian}, however, for manifold defined by our constraints, it is unkonwn. It can be approximated by the solution of the optimisation problem
\begin{align*}
    \widehat{\exp_x\left(v\right)}=\argmin_{x'}\frac{1}{2}\norm{x'-(x+v)}^2\quad \text{subject to}\quad h(x')=0.
\end{align*}
Reformulate to obtain
\begin{align*}
    \argmin_{\delta x}\frac{1}{2}\norm{x+\delta x-(x+v)}^2\quad \text{subject to}\quad h(x+\delta x)=0.
\end{align*}
Expand the constraint and only keep the first order terms \(h(x+\delta x)=h(x)+J(x)\delta x\), and Lagrangian is
\begin{align*}
    \mathcal{L}\left(\delta x,\lambda\right)=\frac{1}{2}\norm{\delta x - v}^2+\lambda^T\left(h(x)+J(x)\delta x\right).
\end{align*}
Now we can solve with KKT conditions as the problem is convex, i.e.,
\begin{align*}
    \nabla_{\delta x}\mathcal{L}\left(\delta x,\lambda\right)&=0\\
    h(x)+J(x)\delta x&=0
\end{align*}
This yields the optimal update
\begin{align*}
    \delta x=v-J^T\left(x\right)\lambda
\end{align*}
where \(\lambda\) solves
\begin{align*}
    J(x)J^T(x)\lambda=h(x)+J(x)v=J(x)v.\tag{\text{since we start on manifold.}}
\end{align*}
In the end the exponential map is
\begin{align*}
    \widehat{\exp_x\left(v\right)}=x+\delta x.
\end{align*}
\subsection{Constrained Symplectic Euler-Like Method}
This is an alternative to geodesic random walk that avoid approximating the exponential map. Instead of trying to sample from \eqref{eq:constrained-sde-tangent-space}, we use the equivalent
\begin{align*}
    dx&=f(x,t)dt+g(t)dw-J^T(x)\lambda(t)dt,\\
    h(x)&=0.
\end{align*}
We use the constrained Symplectic Euler-like method \cite{leimkuhlerMolecularDynamicsDeterministic2015}
\begin{align*}
    x_{k+1}=x_k+f(x_k,t_k)\delta t+g(t_k)\sqrt{\delta t}z_{k+1}-J^T(x_k)\lambda_k\delta t
\end{align*}
where \(\lambda_k\) is solved by the constraints
\begin{align*}
    h\left(x_{k+1}\right)=h\left(x_k+f(x_k,t_k)\delta t+g(t_k)\sqrt{\delta t}z_{k+1}-J^T(x_k)\lambda_k\delta t\right)=0.
\end{align*}
Let \(Q_k=x_k+f(x_k,t_k)\delta t+g(t_k)\sqrt{\delta t}z_{k+1}\) and \(\Lambda_k=\lambda_t\delta t\) we have
\begin{align*}
    h\left(Q_k+J^T(x_k)\Lambda_k\right)=0,
\end{align*}
which can be solved by Gauss-Newton iteration
\begin{align*}
    Q^{\ell}&\coloneq Q_k-J^T\left(x_k\right)\Lambda^{\ell}\\
    \Lambda_k^{\ell+1}&=\Lambda_k^{\ell}+\left[J\left(Q^{\ell}\right)J^T(x_k)\right]^{-1}h\left(Q^{\ell}\right).
\end{align*}

\bibliographystyle{apalike}
\bibliography{references}
\addcontentsline{toc}{chapter}{Bibliography}
\end{document}
